{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing Libraries and Setting auxiliary Variables\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        out = self.layer1.forward(image_dataset_array)              # self.inputs: (9, 4096)     self.output: (9, 128)  image_dataset_array: (9, 4096)\n",
    "        out = self.layer2.forward(out)                              # self.inputs: (9, 128)      self.output: (9, 64)   image_dataset_array: (9, 128)\n",
    "        out = self.layer3.forward(out)                              # self.inputs: (9, 64)       self.output: (9, 1)    image_dataset_array: (9, 64\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dvalues = self.layer3.backward(dvalues)                     # self.dvalues: (9, 1)      self.dweights: (64, 1)      OUTPUT: (9, 64)\n",
    "        dvalues = self.layer2.backward(dvalues)                     # self.dvalues: (9, 64)     self.dweights: (128, 64)    OUTPUT: (9, 128)\n",
    "        self.layer1.backward(dvalues)                               # self.dvalues: (9, 128)    self.dweights: (4096, 128)  OUTPUT: (9, 4096)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gchakshu\\AppData\\Local\\Temp\\ipykernel_3456\\3745664846.py:17: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -----> Epoch: 0 \n",
      "--------->Loss: 0.2520542124472208\n",
      " -----> Epoch: 1 \n",
      "--------->Loss: 0.24716986342457778\n",
      " -----> Epoch: 2 \n",
      "--------->Loss: 0.3005685535365865\n",
      " -----> Epoch: 3 \n",
      "--------->Loss: 0.2613537263476659\n",
      " -----> Epoch: 4 \n",
      "--------->Loss: 0.2550323436072473\n",
      " -----> Epoch: 5 \n",
      "--------->Loss: 0.2634132429895092\n",
      " -----> Epoch: 6 \n",
      "--------->Loss: 0.2206341067372123\n",
      " -----> Epoch: 7 \n",
      "--------->Loss: 0.2585348742969358\n",
      " -----> Epoch: 8 \n",
      "--------->Loss: 0.20509299382449575\n",
      " -----> Epoch: 9 \n",
      "--------->Loss: 0.16870444725424336\n",
      " -----> Epoch: 10 \n",
      "--------->Loss: 0.11479826321840586\n",
      " -----> Epoch: 11 \n",
      "--------->Loss: 0.13168084067417657\n",
      " -----> Epoch: 12 \n",
      "--------->Loss: 0.10860100227654255\n",
      " -----> Epoch: 13 \n",
      "--------->Loss: 0.12502263319363463\n",
      " -----> Epoch: 14 \n",
      "--------->Loss: 0.09910777462629149\n",
      " -----> Epoch: 15 \n",
      "--------->Loss: 0.10063233983296582\n",
      " -----> Epoch: 16 \n",
      "--------->Loss: 0.09671824587215053\n",
      " -----> Epoch: 17 \n",
      "--------->Loss: 0.09136554706708226\n",
      " -----> Epoch: 18 \n",
      "--------->Loss: 0.08712801841901475\n",
      " -----> Epoch: 19 \n",
      "--------->Loss: 0.08344802469958267\n",
      " -----> Epoch: 20 \n",
      "--------->Loss: 0.07958133437432202\n",
      " -----> Epoch: 21 \n",
      "--------->Loss: 0.0754719599255046\n",
      " -----> Epoch: 22 \n",
      "--------->Loss: 0.07257965479791645\n",
      " -----> Epoch: 23 \n",
      "--------->Loss: 0.06913370097492612\n",
      " -----> Epoch: 24 \n",
      "--------->Loss: 0.06602621260171149\n",
      " -----> Epoch: 25 \n",
      "--------->Loss: 0.0628972538931526\n",
      " -----> Epoch: 26 \n",
      "--------->Loss: 0.06021180170444378\n",
      " -----> Epoch: 27 \n",
      "--------->Loss: 0.05781724932440645\n",
      " -----> Epoch: 28 \n",
      "--------->Loss: 0.055387288918037826\n",
      " -----> Epoch: 29 \n",
      "--------->Loss: 0.053238407370235705\n",
      " -----> Epoch: 30 \n",
      "--------->Loss: 0.05118489595297687\n",
      " -----> Epoch: 31 \n",
      "--------->Loss: 0.04914619918329851\n",
      " -----> Epoch: 32 \n",
      "--------->Loss: 0.047259882002639655\n",
      " -----> Epoch: 33 \n",
      "--------->Loss: 0.04542972942775346\n",
      " -----> Epoch: 34 \n",
      "--------->Loss: 0.04371737328038979\n",
      " -----> Epoch: 35 \n",
      "--------->Loss: 0.04199235693844705\n",
      " -----> Epoch: 36 \n",
      "--------->Loss: 0.04039464524010145\n",
      " -----> Epoch: 37 \n",
      "--------->Loss: 0.0388994109574958\n",
      " -----> Epoch: 38 \n",
      "--------->Loss: 0.03748258653652408\n",
      " -----> Epoch: 39 \n",
      "--------->Loss: 0.036387534032731185\n",
      " -----> Epoch: 40 \n",
      "--------->Loss: 0.035193863429466156\n",
      " -----> Epoch: 41 \n",
      "--------->Loss: 0.0339388133311442\n",
      " -----> Epoch: 42 \n",
      "--------->Loss: 0.03274681108663908\n",
      " -----> Epoch: 43 \n",
      "--------->Loss: 0.03159724034897553\n",
      " -----> Epoch: 44 \n",
      "--------->Loss: 0.030716396061848006\n",
      " -----> Epoch: 45 \n",
      "--------->Loss: 0.02965716673413997\n",
      " -----> Epoch: 46 \n",
      "--------->Loss: 0.02864169707779031\n",
      " -----> Epoch: 47 \n",
      "--------->Loss: 0.02773777657556781\n",
      " -----> Epoch: 48 \n",
      "--------->Loss: 0.026832477530825332\n",
      " -----> Epoch: 49 \n",
      "--------->Loss: 0.02593496364015868\n",
      " -----> Epoch: 50 \n",
      "--------->Loss: 0.025213897944369557\n",
      " -----> Epoch: 51 \n",
      "--------->Loss: 0.024484761612388304\n",
      " -----> Epoch: 52 \n",
      "--------->Loss: 0.023772152193986193\n",
      " -----> Epoch: 53 \n",
      "--------->Loss: 0.02300491115753935\n",
      " -----> Epoch: 54 \n",
      "--------->Loss: 0.022250343728920312\n",
      " -----> Epoch: 55 \n",
      "--------->Loss: 0.02158112328426161\n",
      " -----> Epoch: 56 \n",
      "--------->Loss: 0.020900942027382033\n",
      " -----> Epoch: 57 \n",
      "--------->Loss: 0.020227678397884127\n",
      " -----> Epoch: 58 \n",
      "--------->Loss: 0.019792139608076312\n",
      " -----> Epoch: 59 \n",
      "--------->Loss: 0.01895500349441005\n",
      " -----> Epoch: 60 \n",
      "--------->Loss: 0.018413154680409632\n",
      " -----> Epoch: 61 \n",
      "--------->Loss: 0.018037072224018045\n",
      " -----> Epoch: 62 \n",
      "--------->Loss: 0.01731526986868843\n",
      " -----> Epoch: 63 \n",
      "--------->Loss: 0.016973595521310957\n",
      " -----> Epoch: 64 \n",
      "--------->Loss: 0.01625253204237044\n",
      " -----> Epoch: 65 \n",
      "--------->Loss: 0.01607855789279629\n",
      " -----> Epoch: 66 \n",
      "--------->Loss: 0.01531197543524496\n",
      " -----> Epoch: 67 \n",
      "--------->Loss: 0.015118601054457717\n",
      " -----> Epoch: 68 \n",
      "--------->Loss: 0.0144429436981382\n",
      " -----> Epoch: 69 \n",
      "--------->Loss: 0.014269006126051741\n",
      " -----> Epoch: 70 \n",
      "--------->Loss: 0.013865899757929546\n",
      " -----> Epoch: 71 \n",
      "--------->Loss: 0.013191174957800538\n",
      " -----> Epoch: 72 \n",
      "--------->Loss: 0.013105221740230929\n",
      " -----> Epoch: 73 \n",
      "--------->Loss: 0.012743525224866187\n",
      " -----> Epoch: 74 \n",
      "--------->Loss: 0.012340011476080425\n",
      " -----> Epoch: 75 \n",
      "--------->Loss: 0.012004928991581117\n",
      " -----> Epoch: 76 \n",
      "--------->Loss: 0.011681479544740619\n",
      " -----> Epoch: 77 \n",
      "--------->Loss: 0.011369188739265882\n",
      " -----> Epoch: 78 \n",
      "--------->Loss: 0.011067603763402828\n",
      " -----> Epoch: 79 \n",
      "--------->Loss: 0.01077629233359007\n",
      " -----> Epoch: 80 \n",
      "--------->Loss: 0.010494841692292372\n",
      " -----> Epoch: 81 \n",
      "--------->Loss: 0.010222857655406944\n",
      " -----> Epoch: 82 \n",
      "--------->Loss: 0.009959963553702385\n",
      " -----> Epoch: 83 \n",
      "--------->Loss: 0.009705799174744269\n",
      " -----> Epoch: 84 \n",
      "--------->Loss: 0.009524653187783361\n",
      " -----> Epoch: 85 \n",
      "--------->Loss: 0.009210729235992346\n",
      " -----> Epoch: 86 \n",
      "--------->Loss: 0.009069308140576732\n",
      " -----> Epoch: 87 \n",
      "--------->Loss: 0.008758017382141076\n",
      " -----> Epoch: 88 \n",
      "--------->Loss: 0.008596793723305005\n",
      " -----> Epoch: 89 \n",
      "--------->Loss: 0.00838567347152865\n",
      " -----> Epoch: 90 \n",
      "--------->Loss: 0.008132126111299074\n",
      " -----> Epoch: 91 \n",
      "--------->Loss: 0.008261491296199075\n",
      " -----> Epoch: 92 \n",
      "--------->Loss: 0.007513800813398015\n",
      " -----> Epoch: 93 \n",
      "--------->Loss: 0.007878722668531549\n",
      " -----> Epoch: 94 \n",
      "--------->Loss: 0.007189057498926009\n",
      " -----> Epoch: 95 \n",
      "--------->Loss: 0.00751882459366736\n",
      " -----> Epoch: 96 \n",
      "--------->Loss: 0.006856436862995929\n",
      " -----> Epoch: 97 \n",
      "--------->Loss: 0.007180633118252612\n",
      " -----> Epoch: 98 \n",
      "--------->Loss: 0.0065438539997099785\n",
      " -----> Epoch: 99 \n",
      "--------->Loss: 0.0069489935001210805\n",
      " -----> Epoch: 100 \n",
      "--------->Loss: 0.006191197831029389\n",
      " -----> Epoch: 101 \n",
      "--------->Loss: 0.006562753756804098\n",
      " -----> Epoch: 102 \n",
      "--------->Loss: 0.005972954551397017\n",
      " -----> Epoch: 103 \n",
      "--------->Loss: 0.0064496054569772795\n",
      " -----> Epoch: 104 \n",
      "--------->Loss: 0.0055905642644839635\n",
      " -----> Epoch: 105 \n",
      "--------->Loss: 0.006142087435670803\n",
      " -----> Epoch: 106 \n",
      "--------->Loss: 0.005378816673942275\n",
      " -----> Epoch: 107 \n",
      "--------->Loss: 0.005886295733701251\n",
      " -----> Epoch: 108 \n",
      "--------->Loss: 0.005149267858474423\n",
      " -----> Epoch: 109 \n",
      "--------->Loss: 0.00564475114592324\n",
      " -----> Epoch: 110 \n",
      "--------->Loss: 0.004916519831136473\n",
      " -----> Epoch: 111 \n",
      "--------->Loss: 0.0054006182429276045\n",
      " -----> Epoch: 112 \n",
      "--------->Loss: 0.004712764724079725\n",
      " -----> Epoch: 113 \n",
      "--------->Loss: 0.0051853173105285534\n",
      " -----> Epoch: 114 \n",
      "--------->Loss: 0.004520168826294928\n",
      " -----> Epoch: 115 \n",
      "--------->Loss: 0.0049812988212080645\n",
      " -----> Epoch: 116 \n",
      "--------->Loss: 0.004390691740741667\n",
      " -----> Epoch: 117 \n",
      "--------->Loss: 0.004649667234973661\n",
      " -----> Epoch: 118 \n",
      "--------->Loss: 0.004304407269793436\n",
      " -----> Epoch: 119 \n",
      "--------->Loss: 0.004470212900440072\n",
      " -----> Epoch: 120 \n",
      "--------->Loss: 0.004073698205845017\n",
      " -----> Epoch: 121 \n",
      "--------->Loss: 0.004431383033409463\n",
      " -----> Epoch: 122 \n",
      "--------->Loss: 0.0038954903979774285\n",
      " -----> Epoch: 123 \n",
      "--------->Loss: 0.004138828937104248\n",
      " -----> Epoch: 124 \n",
      "--------->Loss: 0.00383326020224822\n",
      " -----> Epoch: 125 \n",
      "--------->Loss: 0.003991193840653627\n",
      " -----> Epoch: 126 \n",
      "--------->Loss: 0.003632734802255209\n",
      " -----> Epoch: 127 \n",
      "--------->Loss: 0.003965903721494294\n",
      " -----> Epoch: 128 \n",
      "--------->Loss: 0.0034780442245139526\n",
      " -----> Epoch: 129 \n",
      "--------->Loss: 0.003706177174646486\n",
      " -----> Epoch: 130 \n",
      "--------->Loss: 0.00342402604797978\n",
      " -----> Epoch: 131 \n",
      "--------->Loss: 0.0035737993389361763\n",
      " -----> Epoch: 132 \n",
      "--------->Loss: 0.0032487185647308934\n",
      " -----> Epoch: 133 \n",
      "--------->Loss: 0.0035519526272991403\n",
      " -----> Epoch: 134 \n",
      "--------->Loss: 0.003114284703905618\n",
      " -----> Epoch: 135 \n",
      "--------->Loss: 0.003436519679229088\n",
      " -----> Epoch: 136 \n",
      "--------->Loss: 0.0029659031532101587\n",
      " -----> Epoch: 137 \n",
      "--------->Loss: 0.0033125670561605413\n",
      " -----> Epoch: 138 \n",
      "--------->Loss: 0.002899925746871166\n",
      " -----> Epoch: 139 \n",
      "--------->Loss: 0.003207603356081973\n",
      " -----> Epoch: 140 \n",
      "--------->Loss: 0.0027638280647017278\n",
      " -----> Epoch: 141 \n",
      "--------->Loss: 0.0030947416190977173\n",
      " -----> Epoch: 142 \n",
      "--------->Loss: 0.002705188257710435\n",
      " -----> Epoch: 143 \n",
      "--------->Loss: 0.002999081576177136\n",
      " -----> Epoch: 144 \n",
      "--------->Loss: 0.0025800981479521274\n",
      " -----> Epoch: 145 \n",
      "--------->Loss: 0.002896105285598219\n",
      " -----> Epoch: 146 \n",
      "--------->Loss: 0.0025278832074211634\n",
      " -----> Epoch: 147 \n",
      "--------->Loss: 0.002808344147996326\n",
      " -----> Epoch: 148 \n",
      "--------->Loss: 0.002445056922318723\n",
      " -----> Epoch: 149 \n",
      "--------->Loss: 0.002671350530077914\n",
      " -----> Epoch: 150 \n",
      "--------->Loss: 0.0023817983449729175\n",
      " -----> Epoch: 151 \n",
      "--------->Loss: 0.002633682008247195\n",
      " -----> Epoch: 152 \n",
      "--------->Loss: 0.002290227964459147\n",
      " -----> Epoch: 153 \n",
      "--------->Loss: 0.0024663550269130173\n",
      " -----> Epoch: 154 \n",
      "--------->Loss: 0.002271446661575111\n",
      " -----> Epoch: 155 \n",
      "--------->Loss: 0.002429788953436844\n",
      " -----> Epoch: 156 \n",
      "--------->Loss: 0.002163524704240604\n",
      " -----> Epoch: 157 \n",
      "--------->Loss: 0.0023991294947595357\n",
      " -----> Epoch: 158 \n",
      "--------->Loss: 0.0020823225934200867\n",
      " -----> Epoch: 159 \n",
      "--------->Loss: 0.002285167994078661\n",
      " -----> Epoch: 160 \n",
      "--------->Loss: 0.0020330461046214766\n",
      " -----> Epoch: 161 \n",
      "--------->Loss: 0.002258506372643236\n",
      " -----> Epoch: 162 \n",
      "--------->Loss: 0.001957875262417225\n",
      " -----> Epoch: 163 \n",
      "--------->Loss: 0.0021923654850930234\n",
      " -----> Epoch: 164 \n",
      "--------->Loss: 0.001887062274311778\n",
      " -----> Epoch: 165 \n",
      "--------->Loss: 0.0021289434933301644\n",
      " -----> Epoch: 166 \n",
      "--------->Loss: 0.001843401290875547\n",
      " -----> Epoch: 167 \n",
      "--------->Loss: 0.002067924101134127\n",
      " -----> Epoch: 168 \n",
      "--------->Loss: 0.0017894670591768484\n",
      " -----> Epoch: 169 \n",
      "--------->Loss: 0.0019386985191241417\n",
      " -----> Epoch: 170 \n",
      "--------->Loss: 0.0017822061491149207\n",
      " -----> Epoch: 171 \n",
      "--------->Loss: 0.0019531855699098434\n",
      " -----> Epoch: 172 \n",
      "--------->Loss: 0.001688339997777771\n",
      " -----> Epoch: 173 \n",
      "--------->Loss: 0.0018628224594918837\n",
      " -----> Epoch: 174 \n",
      "--------->Loss: 0.0016530845080547298\n",
      " -----> Epoch: 175 \n",
      "--------->Loss: 0.0018470991323789989\n",
      " -----> Epoch: 176 \n",
      "--------->Loss: 0.0015948763573035824\n",
      " -----> Epoch: 177 \n",
      "--------->Loss: 0.0017969785848696563\n",
      " -----> Epoch: 178 \n",
      "--------->Loss: 0.0015507823241211701\n",
      " -----> Epoch: 179 \n",
      "--------->Loss: 0.0017487293244180533\n",
      " -----> Epoch: 180 \n",
      "--------->Loss: 0.0015083595715968687\n",
      " -----> Epoch: 181 \n",
      "--------->Loss: 0.0017022634217063522\n",
      " -----> Epoch: 182 \n",
      "--------->Loss: 0.0014624474015697381\n",
      " -----> Epoch: 183 \n",
      "--------->Loss: 0.0016575368616484978\n",
      " -----> Epoch: 184 \n",
      "--------->Loss: 0.0014232877000418748\n",
      " -----> Epoch: 185 \n",
      "--------->Loss: 0.0016144299321887692\n",
      " -----> Epoch: 186 \n",
      "--------->Loss: 0.0013855685427810519\n",
      " -----> Epoch: 187 \n",
      "--------->Loss: 0.0015680946002219155\n",
      " -----> Epoch: 188 \n",
      "--------->Loss: 0.0013492432704063444\n",
      " -----> Epoch: 189 \n",
      "--------->Loss: 0.001526303945367412\n",
      " -----> Epoch: 190 \n",
      "--------->Loss: 0.0013235863864240452\n",
      " -----> Epoch: 191 \n",
      "--------->Loss: 0.0014896228972298082\n",
      " -----> Epoch: 192 \n",
      "--------->Loss: 0.0012804730756909757\n",
      " -----> Epoch: 193 \n",
      "--------->Loss: 0.0014507484493990204\n",
      " -----> Epoch: 194 \n",
      "--------->Loss: 0.0012581407565221221\n",
      " -----> Epoch: 195 \n",
      "--------->Loss: 0.0014165188593399881\n",
      " -----> Epoch: 196 \n",
      "--------->Loss: 0.0012164805944657941\n",
      " -----> Epoch: 197 \n",
      "--------->Loss: 0.0013803121622574382\n",
      " -----> Epoch: 198 \n",
      "--------->Loss: 0.0011960101437521846\n",
      " -----> Epoch: 199 \n",
      "--------->Loss: 0.0013468613855932214\n",
      " -----> Epoch: 200 \n",
      "--------->Loss: 0.0011665384930372074\n",
      " -----> Epoch: 201 \n",
      "--------->Loss: 0.0013159438256633857\n",
      " -----> Epoch: 202 \n",
      "--------->Loss: 0.0011285653699429428\n",
      " -----> Epoch: 203 \n",
      "--------->Loss: 0.001283314603527459\n",
      " -----> Epoch: 204 \n",
      "--------->Loss: 0.0011105658880849018\n",
      " -----> Epoch: 205 \n",
      "--------->Loss: 0.001253091683310203\n",
      " -----> Epoch: 206 \n",
      "--------->Loss: 0.0010839761377022424\n",
      " -----> Epoch: 207 \n",
      "--------->Loss: 0.001225082132235786\n",
      " -----> Epoch: 208 \n",
      "--------->Loss: 0.0010580725494700868\n",
      " -----> Epoch: 209 \n",
      "--------->Loss: 0.001196713327011748\n",
      " -----> Epoch: 210 \n",
      "--------->Loss: 0.001024540646153268\n",
      " -----> Epoch: 211 \n",
      "--------->Loss: 0.0011682083725610922\n",
      " -----> Epoch: 212 \n",
      "--------->Loss: 0.0010093316673905096\n",
      " -----> Epoch: 213 \n",
      "--------->Loss: 0.0011416971004127926\n",
      " -----> Epoch: 214 \n",
      "--------->Loss: 0.0009860516083901344\n",
      " -----> Epoch: 215 \n",
      "--------->Loss: 0.0011170261857970628\n",
      " -----> Epoch: 216 \n",
      "--------->Loss: 0.0009634975103535379\n",
      " -----> Epoch: 217 \n",
      "--------->Loss: 0.0010920869862569528\n",
      " -----> Epoch: 218 \n",
      "--------->Loss: 0.0009334372710526409\n",
      " -----> Epoch: 219 \n",
      "--------->Loss: 0.001067076811994649\n",
      " -----> Epoch: 220 \n",
      "--------->Loss: 0.0009205475795358882\n",
      " -----> Epoch: 221 \n",
      "--------->Loss: 0.0010437185392792592\n",
      " -----> Epoch: 222 \n",
      "--------->Loss: 0.0009000707645987047\n",
      " -----> Epoch: 223 \n",
      "--------->Loss: 0.001021072340868459\n",
      " -----> Epoch: 224 \n",
      "--------->Loss: 0.0008802265672401875\n",
      " -----> Epoch: 225 \n",
      "--------->Loss: 0.0009998776741083526\n",
      " -----> Epoch: 226 \n",
      "--------->Loss: 0.0008609749292826511\n",
      " -----> Epoch: 227 \n",
      "--------->Loss: 0.0009785244095466346\n",
      " -----> Epoch: 228 \n",
      "--------->Loss: 0.0008423086640526368\n",
      " -----> Epoch: 229 \n",
      "--------->Loss: 0.0009578007489825899\n",
      " -----> Epoch: 230 \n",
      "--------->Loss: 0.0008242053309299312\n",
      " -----> Epoch: 231 \n",
      "--------->Loss: 0.0009376892740347126\n",
      " -----> Epoch: 232 \n",
      "--------->Loss: 0.0008066417871108797\n",
      " -----> Epoch: 233 \n",
      "--------->Loss: 0.0009181670748288326\n",
      " -----> Epoch: 234 \n",
      "--------->Loss: 0.000784122012105441\n",
      " -----> Epoch: 235 \n",
      "--------->Loss: 0.0008986899841716067\n",
      " -----> Epoch: 236 \n",
      "--------->Loss: 0.00077309069570527\n",
      " -----> Epoch: 237 \n",
      "--------->Loss: 0.0008803247754802391\n",
      " -----> Epoch: 238 \n",
      "--------->Loss: 0.0007570455160554418\n",
      " -----> Epoch: 239 \n",
      "--------->Loss: 0.0008624790568535252\n",
      " -----> Epoch: 240 \n",
      "--------->Loss: 0.0007414643557806744\n",
      " -----> Epoch: 241 \n",
      "--------->Loss: 0.000845149176705535\n",
      " -----> Epoch: 242 \n",
      "--------->Loss: 0.0007263304041043497\n",
      " -----> Epoch: 243 \n",
      "--------->Loss: 0.0008283086790870552\n",
      " -----> Epoch: 244 \n",
      "--------->Loss: 0.0007116274283278699\n",
      " -----> Epoch: 245 \n",
      "--------->Loss: 0.0008119332659612595\n",
      " -----> Epoch: 246 \n",
      "--------->Loss: 0.0006973397979066757\n",
      " -----> Epoch: 247 \n",
      "--------->Loss: 0.0007960103037539024\n",
      " -----> Epoch: 248 \n",
      "--------->Loss: 0.0006834526275752072\n",
      " -----> Epoch: 249 \n",
      "--------->Loss: 0.0007805245594711463\n",
      " -----> Epoch: 250 \n",
      "--------->Loss: 0.0006699516832485997\n",
      " -----> Epoch: 251 \n",
      "--------->Loss: 0.0007654607911506903\n",
      " -----> Epoch: 252 \n",
      "--------->Loss: 0.0006568233390698375\n",
      " -----> Epoch: 253 \n",
      "--------->Loss: 0.000750804380822867\n",
      " -----> Epoch: 254 \n",
      "--------->Loss: 0.000644054547003363\n",
      " -----> Epoch: 255 \n",
      "--------->Loss: 0.0007365413201860502\n",
      " -----> Epoch: 256 \n",
      "--------->Loss: 0.0006316328083822256\n",
      " -----> Epoch: 257 \n",
      "--------->Loss: 0.0007226581811074621\n",
      " -----> Epoch: 258 \n",
      "--------->Loss: 0.0006195461470442419\n",
      " -----> Epoch: 259 \n",
      "--------->Loss: 0.0007091420876769644\n",
      " -----> Epoch: 260 \n",
      "--------->Loss: 0.0006077835704346375\n",
      " -----> Epoch: 261 \n",
      "--------->Loss: 0.0006959806872449749\n",
      " -----> Epoch: 262 \n",
      "--------->Loss: 0.0006084459997552664\n",
      " -----> Epoch: 263 \n",
      "--------->Loss: 0.0006559807855887867\n",
      " -----> Epoch: 264 \n",
      "--------->Loss: 0.0006056956620449754\n",
      " -----> Epoch: 265 \n",
      "--------->Loss: 0.0006708416165231579\n",
      " -----> Epoch: 266 \n",
      "--------->Loss: 0.0005742794964117847\n",
      " -----> Epoch: 267 \n",
      "--------->Loss: 0.0006586490132620907\n",
      " -----> Epoch: 268 \n",
      "--------->Loss: 0.0005636987132641065\n",
      " -----> Epoch: 269 \n",
      "--------->Loss: 0.0006467676207160112\n",
      " -----> Epoch: 270 \n",
      "--------->Loss: 0.0005533911926139872\n",
      " -----> Epoch: 271 \n",
      "--------->Loss: 0.0006351156063529288\n",
      " -----> Epoch: 272 \n",
      "--------->Loss: 0.000543348679081611\n",
      " -----> Epoch: 273 \n",
      "--------->Loss: 0.0006237726237647834\n",
      " -----> Epoch: 274 \n",
      "--------->Loss: 0.0005335682090784295\n",
      " -----> Epoch: 275 \n",
      "--------->Loss: 0.0006127893583294264\n",
      " -----> Epoch: 276 \n",
      "--------->Loss: 0.0005240347200914819\n",
      " -----> Epoch: 277 \n",
      "--------->Loss: 0.0006020781132460367\n",
      " -----> Epoch: 278 \n",
      "--------->Loss: 0.0005253379068657811\n",
      " -----> Epoch: 279 \n",
      "--------->Loss: 0.0005686818842108758\n",
      " -----> Epoch: 280 \n",
      "--------->Loss: 0.0005236957217156476\n",
      " -----> Epoch: 281 \n",
      "--------->Loss: 0.0005814617127241558\n",
      " -----> Epoch: 282 \n",
      "--------->Loss: 0.0004967972308561941\n",
      " -----> Epoch: 283 \n",
      "--------->Loss: 0.0005714969451829407\n",
      " -----> Epoch: 284 \n",
      "--------->Loss: 0.00048817022931299205\n",
      " -----> Epoch: 285 \n",
      "--------->Loss: 0.0005616298918374177\n",
      " -----> Epoch: 286 \n",
      "--------->Loss: 0.0004797587732517838\n",
      " -----> Epoch: 287 \n",
      "--------->Loss: 0.0005522659199680223\n",
      " -----> Epoch: 288 \n",
      "--------->Loss: 0.00047155258683446316\n",
      " -----> Epoch: 289 \n",
      "--------->Loss: 0.0005430186978708932\n",
      " -----> Epoch: 290 \n",
      "--------->Loss: 0.0004731715066588806\n",
      " -----> Epoch: 291 \n",
      "--------->Loss: 0.0005217256195590362\n",
      " -----> Epoch: 292 \n",
      "--------->Loss: 0.00047212726400150596\n",
      " -----> Epoch: 293 \n",
      "--------->Loss: 0.000502786148277293\n",
      " -----> Epoch: 294 \n",
      "--------->Loss: 0.0004642418447114883\n",
      " -----> Epoch: 295 \n",
      "--------->Loss: 0.0004944874554737209\n",
      " -----> Epoch: 296 \n",
      "--------->Loss: 0.0004565418347619869\n",
      " -----> Epoch: 297 \n",
      "--------->Loss: 0.0005080566058261273\n",
      " -----> Epoch: 298 \n",
      "--------->Loss: 0.00043330621811709746\n",
      " -----> Epoch: 299 \n",
      "--------->Loss: 0.0004998780076167807\n"
     ]
    }
   ],
   "source": [
    "def load_image(path, img_size=(64,64)): \n",
    "    images = []\n",
    "    labels = []\n",
    "    for label, class_dir in enumerate( ['cat' , 'dog'] ):\n",
    "        class_path = os.path.join(path, class_dir)\n",
    "\n",
    "        for img_name in os.listdir(class_path):\n",
    "            img_path = os.path.join(class_path, img_name)\n",
    "            img = Image.open(img_path).convert('L').resize(img_size)\n",
    "            images.append(np.array(img).flatten())\n",
    "            labels.append(label)\n",
    "\n",
    "    return np.array(images) , np.array(labels)\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1- sigmoid(x))\n",
    "\n",
    "class DenseLayer:\n",
    "    def __init__(self, input_size, output_size): #input_size = no. of rows ------ # output = no. of output neurons\n",
    "        self.weights = np.random.randn(input_size, output_size) * 0.1\n",
    "        self.bias = np.zeros((1, output_size))\n",
    "\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.input = inputs\n",
    "        self.output = sigmoid(np.dot(inputs, self.weights) + self.bias )\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, dvalues):\n",
    "        self.dweights = np.dot(self.input.T, dvalues)\n",
    "        self.dbias = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "        return self.dinputs\n",
    "    \n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, image_size_flatten):\n",
    "        self.layer1 = DenseLayer(image_size_flatten, 128)\n",
    "        self.layer2 = DenseLayer(128, 64)\n",
    "        self.layer3 = DenseLayer(64,1)\n",
    "\n",
    "    def forward(self, image_dataset_array):\n",
    "        out = self.layer1.forward(image_dataset_array)              # self.inputs: (9, 4096)     self.output: (9, 128)  image_dataset_array: (9, 4096)\n",
    "        out = self.layer2.forward(out)                              # self.inputs: (9, 128)      self.output: (9, 64)   image_dataset_array: (9, 128)\n",
    "        out = self.layer3.forward(out)                              # self.inputs: (9, 64)       self.output: (9, 1)    image_dataset_array: (9, 64\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dvalues):\n",
    "        dvalues = self.layer3.backward(dvalues)                     # self.dvalues: (9, 1)      self.dweights: (64, 1)      OUTPUT: (9, 64)\n",
    "        dvalues = self.layer2.backward(dvalues)                     # self.dvalues: (9, 64)     self.dweights: (128, 64)    OUTPUT: (9, 128)\n",
    "        self.layer1.backward(dvalues)                               # self.dvalues: (9, 128)    self.dweights: (4096, 128)  OUTPUT: (9, 4096)\n",
    "\n",
    "    def update(self, learning_rate):\n",
    "        self.layer1.weights -= learning_rate * self.layer1.dweights\n",
    "        self.layer1.bias -= learning_rate * self.layer1.dbias\n",
    "        \n",
    "        self.layer2.weights -= learning_rate * self.layer2.dweights\n",
    "        self.layer2.bias -= learning_rate * self.layer2.dbias\n",
    "        \n",
    "        self.layer3.weights -= learning_rate * self.layer3.dweights\n",
    "        self.layer3.bias -= learning_rate * self.layer3.dbias\n",
    "\n",
    "\n",
    "def train(model, image_dataset_array, actual_labels, epochs, learning_rate):\n",
    "    for epoch in range(epochs):\n",
    "        output = model.forward(image_dataset_array)\n",
    "        loss = np.mean((output-actual_labels)**2 )\n",
    "        print(f\" -----> Epoch: {epoch} \\n--------->Loss: {loss}\")\n",
    "\n",
    "        dvalues = 2* (output-actual_labels) / actual_labels.size\n",
    "        model.backward(dvalues)\n",
    "        model.update(learning_rate)\n",
    "\n",
    "\n",
    "\n",
    "image_dataset_array, actual_labels = load_image('dataset')\n",
    "actual_labels = actual_labels.reshape(-1, 1)\n",
    "\n",
    "input_image_flat_size = image_dataset_array.shape[1]\n",
    "\n",
    "\n",
    "\n",
    "model1 = NeuralNetwork(image_size_flatten=input_image_flat_size)\n",
    "\n",
    "\n",
    "train(model1, image_dataset_array, actual_labels, epochs=300, learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 40.00%\n",
      "Number of images - 5\n",
      "Precision: 0.40\n",
      "Recall: 1.00\n",
      "F1 Score: 0.57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gchakshu\\AppData\\Local\\Temp\\ipykernel_3456\\3745664846.py:17: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-x))\n"
     ]
    }
   ],
   "source": [
    "def load_mixed_images(path, img_size=(64, 64)):\n",
    "    images = []\n",
    "    actual_labels = []\n",
    "    for img_name in os.listdir(path):\n",
    "        img_path = os.path.join(path, img_name)\n",
    "        img = Image.open(img_path).convert('L').resize(img_size)\n",
    "        images.append(np.array(img).flatten())\n",
    "        label = 1 if 'dog' in img_name.lower() else 0  # Assuming naming convention indicates label\n",
    "        actual_labels.append(label)\n",
    "    return np.array(images), np.array(actual_labels)\n",
    "\n",
    "# Define a function to calculate accuracy\n",
    "def calculate_accuracy(predictions, actual_labels):\n",
    "    correct_predictions = (predictions.flatten() > 0.5).astype(int) == actual_labels\n",
    "    accuracy = np.mean(correct_predictions)\n",
    "    return accuracy\n",
    "\n",
    "# Define a function for precision, recall, and F1-score\n",
    "def calculate_precision_recall_f1(predictions, actual_labels):\n",
    "    predictions_binary = (predictions.flatten() > 0.5).astype(int)\n",
    "    true_positives = np.sum((predictions_binary == 1) & (actual_labels == 1))\n",
    "    false_positives = np.sum((predictions_binary == 1) & (actual_labels == 0))\n",
    "    false_negatives = np.sum((predictions_binary == 0) & (actual_labels == 1))\n",
    "\n",
    "    precision = true_positives / (true_positives + false_positives)\n",
    "    recall = true_positives / (true_positives + false_negatives)\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    return precision, recall, f1_score\n",
    "\n",
    "# Load the mixed dataset\n",
    "X_mixed, y_actual = load_mixed_images('test_set')\n",
    "\n",
    "# Get model predictions\n",
    "predictions = model1.forward(X_mixed)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = calculate_accuracy(predictions, y_actual)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "# Calculate precision, recall, and F1-score\n",
    "precision, recall, f1_score = calculate_precision_recall_f1(predictions, y_actual)\n",
    "print(f\"Number of images - {sum(os.path.isfile(os.path.join('test_set', f)) for f in os.listdir('test_set'))}\")\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'F1 Score: {f1_score:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CleanNNenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
