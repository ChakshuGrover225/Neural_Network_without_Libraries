{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing Libraries and Setting auxiliary Variables\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gchakshu\\AppData\\Local\\Temp\\ipykernel_3456\\3745664846.py:17: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -----> Epoch: 0 \n",
      "--------->Loss: 0.24253981670532043\n",
      " -----> Epoch: 1 \n",
      "--------->Loss: 0.26379020186276547\n",
      " -----> Epoch: 2 \n",
      "--------->Loss: 0.2952041468423206\n",
      " -----> Epoch: 3 \n",
      "--------->Loss: 0.2689311553743247\n",
      " -----> Epoch: 4 \n",
      "--------->Loss: 0.2328225494891022\n",
      " -----> Epoch: 5 \n",
      "--------->Loss: 0.12277233105073877\n",
      " -----> Epoch: 6 \n",
      "--------->Loss: 0.19154957230114256\n",
      " -----> Epoch: 7 \n",
      "--------->Loss: 0.11126612533606431\n",
      " -----> Epoch: 8 \n",
      "--------->Loss: 0.1740112291882966\n",
      " -----> Epoch: 9 \n",
      "--------->Loss: 0.10224433909395349\n",
      " -----> Epoch: 10 \n",
      "--------->Loss: 0.15265373397284973\n",
      " -----> Epoch: 11 \n",
      "--------->Loss: 0.09391898599612557\n",
      " -----> Epoch: 12 \n",
      "--------->Loss: 0.14621295120112363\n",
      " -----> Epoch: 13 \n",
      "--------->Loss: 0.08793110070705056\n",
      " -----> Epoch: 14 \n",
      "--------->Loss: 0.1306018436257649\n",
      " -----> Epoch: 15 \n",
      "--------->Loss: 0.08222910639415298\n",
      " -----> Epoch: 16 \n",
      "--------->Loss: 0.13711413843917813\n",
      " -----> Epoch: 17 \n",
      "--------->Loss: 0.07697756790764586\n",
      " -----> Epoch: 18 \n",
      "--------->Loss: 0.10028832765617839\n",
      " -----> Epoch: 19 \n",
      "--------->Loss: 0.07205895910111332\n",
      " -----> Epoch: 20 \n",
      "--------->Loss: 0.15457167956076814\n",
      " -----> Epoch: 21 \n",
      "--------->Loss: 0.0705704494163976\n",
      " -----> Epoch: 22 \n",
      "--------->Loss: 0.06437409212913746\n",
      " -----> Epoch: 23 \n",
      "--------->Loss: 0.12033381770786265\n",
      " -----> Epoch: 24 \n",
      "--------->Loss: 0.0604423176787326\n",
      " -----> Epoch: 25 \n",
      "--------->Loss: 0.059470254226746726\n",
      " -----> Epoch: 26 \n",
      "--------->Loss: 0.13376554337207217\n",
      " -----> Epoch: 27 \n",
      "--------->Loss: 0.055605866340322746\n",
      " -----> Epoch: 28 \n",
      "--------->Loss: 0.05243540687135698\n",
      " -----> Epoch: 29 \n",
      "--------->Loss: 0.08718387974570113\n",
      " -----> Epoch: 30 \n",
      "--------->Loss: 0.04866491299114264\n",
      " -----> Epoch: 31 \n",
      "--------->Loss: 0.0548098579345287\n",
      " -----> Epoch: 32 \n",
      "--------->Loss: 0.09913914036664678\n",
      " -----> Epoch: 33 \n",
      "--------->Loss: 0.04383396145732394\n",
      " -----> Epoch: 34 \n",
      "--------->Loss: 0.04213882633815776\n",
      " -----> Epoch: 35 \n",
      "--------->Loss: 0.08766237135273393\n",
      " -----> Epoch: 36 \n",
      "--------->Loss: 0.03916892782622458\n",
      " -----> Epoch: 37 \n",
      "--------->Loss: 0.037865638641523515\n",
      " -----> Epoch: 38 \n",
      "--------->Loss: 0.09080392095338967\n",
      " -----> Epoch: 39 \n",
      "--------->Loss: 0.035358826413662316\n",
      " -----> Epoch: 40 \n",
      "--------->Loss: 0.03389450119942185\n",
      " -----> Epoch: 41 \n",
      "--------->Loss: 0.03427978629805322\n",
      " -----> Epoch: 42 \n",
      "--------->Loss: 0.10850929088141818\n",
      " -----> Epoch: 43 \n",
      "--------->Loss: 0.03188488376562282\n",
      " -----> Epoch: 44 \n",
      "--------->Loss: 0.029610156160890577\n",
      " -----> Epoch: 45 \n",
      "--------->Loss: 0.028807117049573623\n",
      " -----> Epoch: 46 \n",
      "--------->Loss: 0.031279615371921025\n",
      " -----> Epoch: 47 \n",
      "--------->Loss: 0.10137283276018844\n",
      " -----> Epoch: 48 \n",
      "--------->Loss: 0.02713020778795901\n",
      " -----> Epoch: 49 \n",
      "--------->Loss: 0.025129785267230698\n",
      " -----> Epoch: 50 \n",
      "--------->Loss: 0.02432132662088107\n",
      " -----> Epoch: 51 \n",
      "--------->Loss: 0.024557559012355217\n",
      " -----> Epoch: 52 \n",
      "--------->Loss: 0.10341041463197573\n",
      " -----> Epoch: 53 \n",
      "--------->Loss: 0.02391543024310716\n",
      " -----> Epoch: 54 \n",
      "--------->Loss: 0.021497910450877462\n",
      " -----> Epoch: 55 \n",
      "--------->Loss: 0.020835369413734484\n",
      " -----> Epoch: 56 \n",
      "--------->Loss: 0.02042702486301308\n",
      " -----> Epoch: 57 \n",
      "--------->Loss: 0.04461122837466944\n",
      " -----> Epoch: 58 \n",
      "--------->Loss: 0.018784327830471736\n",
      " -----> Epoch: 59 \n",
      "--------->Loss: 0.018402909073074656\n",
      " -----> Epoch: 60 \n",
      "--------->Loss: 0.03783324076634692\n",
      " -----> Epoch: 61 \n",
      "--------->Loss: 0.01702742422350318\n",
      " -----> Epoch: 62 \n",
      "--------->Loss: 0.01687564152842529\n",
      " -----> Epoch: 63 \n",
      "--------->Loss: 0.07209745880714685\n",
      " -----> Epoch: 64 \n",
      "--------->Loss: 0.015623372502216015\n",
      " -----> Epoch: 65 \n",
      "--------->Loss: 0.015000274787247074\n",
      " -----> Epoch: 66 \n",
      "--------->Loss: 0.01455828936548386\n",
      " -----> Epoch: 67 \n",
      "--------->Loss: 0.01428323952989981\n",
      " -----> Epoch: 68 \n",
      "--------->Loss: 0.01565878146868201\n",
      " -----> Epoch: 69 \n",
      "--------->Loss: 0.07016565141355453\n",
      " -----> Epoch: 70 \n",
      "--------->Loss: 0.013056214317716391\n",
      " -----> Epoch: 71 \n",
      "--------->Loss: 0.01255024772470363\n",
      " -----> Epoch: 72 \n",
      "--------->Loss: 0.012131802680890095\n",
      " -----> Epoch: 73 \n",
      "--------->Loss: 0.011874899961200957\n",
      " -----> Epoch: 74 \n",
      "--------->Loss: 0.01172226617641197\n",
      " -----> Epoch: 75 \n",
      "--------->Loss: 0.03553003630652374\n",
      " -----> Epoch: 76 \n",
      "--------->Loss: 0.01077604827417748\n",
      " -----> Epoch: 77 \n",
      "--------->Loss: 0.01055590904483339\n",
      " -----> Epoch: 78 \n",
      "--------->Loss: 0.010417380658451604\n",
      " -----> Epoch: 79 \n",
      "--------->Loss: 0.013643448815201529\n",
      " -----> Epoch: 80 \n",
      "--------->Loss: 0.010714995791242906\n",
      " -----> Epoch: 81 \n",
      "--------->Loss: 0.03771557073446127\n",
      " -----> Epoch: 82 \n",
      "--------->Loss: 0.00910322647255641\n",
      " -----> Epoch: 83 \n",
      "--------->Loss: 0.008817461005054091\n",
      " -----> Epoch: 84 \n",
      "--------->Loss: 0.008659186797646887\n",
      " -----> Epoch: 85 \n",
      "--------->Loss: 0.00859471935330645\n",
      " -----> Epoch: 86 \n",
      "--------->Loss: 0.021669114877970363\n",
      " -----> Epoch: 87 \n",
      "--------->Loss: 0.007937469709385393\n",
      " -----> Epoch: 88 \n",
      "--------->Loss: 0.007825120574131893\n",
      " -----> Epoch: 89 \n",
      "--------->Loss: 0.008030269640623311\n",
      " -----> Epoch: 90 \n",
      "--------->Loss: 0.08404055910784333\n",
      " -----> Epoch: 91 \n",
      "--------->Loss: 0.008572649142624974\n",
      " -----> Epoch: 92 \n",
      "--------->Loss: 0.007158311367128453\n",
      " -----> Epoch: 93 \n",
      "--------->Loss: 0.006917868423204274\n",
      " -----> Epoch: 94 \n",
      "--------->Loss: 0.006726183356694237\n",
      " -----> Epoch: 95 \n",
      "--------->Loss: 0.006530403129545589\n",
      " -----> Epoch: 96 \n",
      "--------->Loss: 0.006405748074642174\n",
      " -----> Epoch: 97 \n",
      "--------->Loss: 0.006285298252726683\n",
      " -----> Epoch: 98 \n",
      "--------->Loss: 0.006331353180916692\n",
      " -----> Epoch: 99 \n",
      "--------->Loss: 0.01102079952944508\n",
      " -----> Epoch: 100 \n",
      "--------->Loss: 0.005941322776822669\n",
      " -----> Epoch: 101 \n",
      "--------->Loss: 0.0065989815872444435\n",
      " -----> Epoch: 102 \n",
      "--------->Loss: 0.010437693816570556\n",
      " -----> Epoch: 103 \n",
      "--------->Loss: 0.005524716696646212\n",
      " -----> Epoch: 104 \n",
      "--------->Loss: 0.005967059662902162\n",
      " -----> Epoch: 105 \n",
      "--------->Loss: 0.01299171507430839\n",
      " -----> Epoch: 106 \n",
      "--------->Loss: 0.005043632107299193\n",
      " -----> Epoch: 107 \n",
      "--------->Loss: 0.005031294339800882\n",
      " -----> Epoch: 108 \n",
      "--------->Loss: 0.005631892903539487\n",
      " -----> Epoch: 109 \n",
      "--------->Loss: 0.009304077594123057\n",
      " -----> Epoch: 110 \n",
      "--------->Loss: 0.004696834329584525\n",
      " -----> Epoch: 111 \n",
      "--------->Loss: 0.004823778230421467\n",
      " -----> Epoch: 112 \n",
      "--------->Loss: 0.019376610631766232\n",
      " -----> Epoch: 113 \n",
      "--------->Loss: 0.004257182113329024\n",
      " -----> Epoch: 114 \n",
      "--------->Loss: 0.004170615825687901\n",
      " -----> Epoch: 115 \n",
      "--------->Loss: 0.004123582641707497\n",
      " -----> Epoch: 116 \n",
      "--------->Loss: 0.004138111031264879\n",
      " -----> Epoch: 117 \n",
      "--------->Loss: 0.005592331962749959\n",
      " -----> Epoch: 118 \n",
      "--------->Loss: 0.004173174198719825\n",
      " -----> Epoch: 119 \n",
      "--------->Loss: 0.018583259675844118\n",
      " -----> Epoch: 120 \n",
      "--------->Loss: 0.0036661111246992697\n",
      " -----> Epoch: 121 \n",
      "--------->Loss: 0.0035824897410082716\n",
      " -----> Epoch: 122 \n",
      "--------->Loss: 0.0035605372723185413\n",
      " -----> Epoch: 123 \n",
      "--------->Loss: 0.003568299791236722\n",
      " -----> Epoch: 124 \n",
      "--------->Loss: 0.0036906164903242867\n",
      " -----> Epoch: 125 \n",
      "--------->Loss: 0.013517167825426217\n",
      " -----> Epoch: 126 \n",
      "--------->Loss: 0.003236893518425981\n",
      " -----> Epoch: 127 \n",
      "--------->Loss: 0.003196475551531715\n",
      " -----> Epoch: 128 \n",
      "--------->Loss: 0.0031866818762468097\n",
      " -----> Epoch: 129 \n",
      "--------->Loss: 0.003264684425408642\n",
      " -----> Epoch: 130 \n",
      "--------->Loss: 0.007199956507264395\n",
      " -----> Epoch: 131 \n",
      "--------->Loss: 0.0029830452062130004\n",
      " -----> Epoch: 132 \n",
      "--------->Loss: 0.0030005993667969275\n",
      " -----> Epoch: 133 \n",
      "--------->Loss: 0.004484812594487535\n",
      " -----> Epoch: 134 \n",
      "--------->Loss: 0.0029657220741519746\n",
      " -----> Epoch: 135 \n",
      "--------->Loss: 0.004688955981211942\n",
      " -----> Epoch: 136 \n",
      "--------->Loss: 0.00279504370173822\n",
      " -----> Epoch: 137 \n",
      "--------->Loss: 0.004327230918593074\n",
      " -----> Epoch: 138 \n",
      "--------->Loss: 0.0027243785122649995\n",
      " -----> Epoch: 139 \n",
      "--------->Loss: 0.004192370473046765\n",
      " -----> Epoch: 140 \n",
      "--------->Loss: 0.00262603295395232\n",
      " -----> Epoch: 141 \n",
      "--------->Loss: 0.0040633717478723115\n",
      " -----> Epoch: 142 \n",
      "--------->Loss: 0.002518847557200907\n",
      " -----> Epoch: 143 \n",
      "--------->Loss: 0.004087723893125065\n",
      " -----> Epoch: 144 \n",
      "--------->Loss: 0.0024148600076545584\n",
      " -----> Epoch: 145 \n",
      "--------->Loss: 0.0035904181387273015\n",
      " -----> Epoch: 146 \n",
      "--------->Loss: 0.00238318056898315\n",
      " -----> Epoch: 147 \n",
      "--------->Loss: 0.00398546026013499\n",
      " -----> Epoch: 148 \n",
      "--------->Loss: 0.0022391725549524583\n",
      " -----> Epoch: 149 \n",
      "--------->Loss: 0.003061294410655639\n",
      " -----> Epoch: 150 \n",
      "--------->Loss: 0.002328834036436531\n",
      " -----> Epoch: 151 \n",
      "--------->Loss: 0.005241460149927985\n",
      " -----> Epoch: 152 \n",
      "--------->Loss: 0.0020369874536618534\n",
      " -----> Epoch: 153 \n",
      "--------->Loss: 0.002049402850068136\n",
      " -----> Epoch: 154 \n",
      "--------->Loss: 0.002618562392453571\n",
      " -----> Epoch: 155 \n",
      "--------->Loss: 0.0023403739212207986\n",
      " -----> Epoch: 156 \n",
      "--------->Loss: 0.0030468731564334195\n",
      " -----> Epoch: 157 \n",
      "--------->Loss: 0.0019688857550119775\n",
      " -----> Epoch: 158 \n",
      "--------->Loss: 0.003167606550264223\n",
      " -----> Epoch: 159 \n",
      "--------->Loss: 0.0018735118676363168\n",
      " -----> Epoch: 160 \n",
      "--------->Loss: 0.0028774848366665104\n",
      " -----> Epoch: 161 \n",
      "--------->Loss: 0.0018459940591480863\n",
      " -----> Epoch: 162 \n",
      "--------->Loss: 0.0029991960201720294\n",
      " -----> Epoch: 163 \n",
      "--------->Loss: 0.0017577125479061877\n",
      " -----> Epoch: 164 \n",
      "--------->Loss: 0.002585452230431956\n",
      " -----> Epoch: 165 \n",
      "--------->Loss: 0.001772634026966866\n",
      " -----> Epoch: 166 \n",
      "--------->Loss: 0.0028443475780205426\n",
      " -----> Epoch: 167 \n",
      "--------->Loss: 0.001651575549701538\n",
      " -----> Epoch: 168 \n",
      "--------->Loss: 0.002446041076209002\n",
      " -----> Epoch: 169 \n",
      "--------->Loss: 0.0016672952281731484\n",
      " -----> Epoch: 170 \n",
      "--------->Loss: 0.002699690036731444\n",
      " -----> Epoch: 171 \n",
      "--------->Loss: 0.0015541221489313922\n",
      " -----> Epoch: 172 \n",
      "--------->Loss: 0.0021773179226462196\n",
      " -----> Epoch: 173 \n",
      "--------->Loss: 0.0015891726567517446\n",
      " -----> Epoch: 174 \n",
      "--------->Loss: 0.0034882339816578534\n",
      " -----> Epoch: 175 \n",
      "--------->Loss: 0.0014253020945716578\n",
      " -----> Epoch: 176 \n",
      "--------->Loss: 0.0014535196589028894\n",
      " -----> Epoch: 177 \n",
      "--------->Loss: 0.002170522706787776\n",
      " -----> Epoch: 178 \n",
      "--------->Loss: 0.0014438332463359688\n",
      " -----> Epoch: 179 \n",
      "--------->Loss: 0.0024043944790353897\n",
      " -----> Epoch: 180 \n",
      "--------->Loss: 0.0013575105483488283\n",
      " -----> Epoch: 181 \n",
      "--------->Loss: 0.0018651532520751522\n",
      " -----> Epoch: 182 \n",
      "--------->Loss: 0.0014443094300785522\n",
      " -----> Epoch: 183 \n",
      "--------->Loss: 0.0024120180276228573\n",
      " -----> Epoch: 184 \n",
      "--------->Loss: 0.0012757614665850124\n",
      " -----> Epoch: 185 \n",
      "--------->Loss: 0.0015065257325516763\n",
      " -----> Epoch: 186 \n",
      "--------->Loss: 0.0020332331337460847\n",
      " -----> Epoch: 187 \n",
      "--------->Loss: 0.0012442009937712602\n",
      " -----> Epoch: 188 \n",
      "--------->Loss: 0.0019848284862208037\n",
      " -----> Epoch: 189 \n",
      "--------->Loss: 0.0012244660792803655\n",
      " -----> Epoch: 190 \n",
      "--------->Loss: 0.001937892261365883\n",
      " -----> Epoch: 191 \n",
      "--------->Loss: 0.0011785479510973348\n",
      " -----> Epoch: 192 \n",
      "--------->Loss: 0.0018926780342795279\n",
      " -----> Epoch: 193 \n",
      "--------->Loss: 0.0011475441535299343\n",
      " -----> Epoch: 194 \n",
      "--------->Loss: 0.0018489548025469862\n",
      " -----> Epoch: 195 \n",
      "--------->Loss: 0.0011176689405540195\n",
      " -----> Epoch: 196 \n",
      "--------->Loss: 0.0018066598572826983\n",
      " -----> Epoch: 197 \n",
      "--------->Loss: 0.0010888706048313278\n",
      " -----> Epoch: 198 \n",
      "--------->Loss: 0.0017657336673407838\n",
      " -----> Epoch: 199 \n",
      "--------->Loss: 0.001061100268695932\n",
      " -----> Epoch: 200 \n",
      "--------->Loss: 0.0016396340369969262\n",
      " -----> Epoch: 201 \n",
      "--------->Loss: 0.0010584596532222238\n",
      " -----> Epoch: 202 \n",
      "--------->Loss: 0.0016879019895544723\n",
      " -----> Epoch: 203 \n",
      "--------->Loss: 0.0010084373112742254\n",
      " -----> Epoch: 204 \n",
      "--------->Loss: 0.001566742110923918\n",
      " -----> Epoch: 205 \n",
      "--------->Loss: 0.0010065700612161077\n",
      " -----> Epoch: 206 \n",
      "--------->Loss: 0.0016148909904948644\n",
      " -----> Epoch: 207 \n",
      "--------->Loss: 0.0009593669889838949\n",
      " -----> Epoch: 208 \n",
      "--------->Loss: 0.0014983829786164669\n",
      " -----> Epoch: 209 \n",
      "--------->Loss: 0.0009470339110504042\n",
      " -----> Epoch: 210 \n",
      "--------->Loss: 0.001546439280872973\n",
      " -----> Epoch: 211 \n",
      "--------->Loss: 0.0009136106435082838\n",
      " -----> Epoch: 212 \n",
      "--------->Loss: 0.001434304421745133\n",
      " -----> Epoch: 213 \n",
      "--------->Loss: 0.00090234864294989\n",
      " -----> Epoch: 214 \n",
      "--------->Loss: 0.0014820732303066331\n",
      " -----> Epoch: 215 \n",
      "--------->Loss: 0.0008673614732514182\n",
      " -----> Epoch: 216 \n",
      "--------->Loss: 0.001327712476888905\n",
      " -----> Epoch: 217 \n",
      "--------->Loss: 0.0008763350552747557\n",
      " -----> Epoch: 218 \n",
      "--------->Loss: 0.0014215811014450042\n",
      " -----> Epoch: 219 \n",
      "--------->Loss: 0.0008275127348753183\n",
      " -----> Epoch: 220 \n",
      "--------->Loss: 0.0012514821200999482\n",
      " -----> Epoch: 221 \n",
      "--------->Loss: 0.0008366630088710048\n",
      " -----> Epoch: 222 \n",
      "--------->Loss: 0.0014285103443856583\n",
      " -----> Epoch: 223 \n",
      "--------->Loss: 0.0007845732042272022\n",
      " -----> Epoch: 224 \n",
      "--------->Loss: 0.0010611100800045907\n",
      " -----> Epoch: 225 \n",
      "--------->Loss: 0.0008769154271074884\n",
      " -----> Epoch: 226 \n",
      "--------->Loss: 0.0012389060947824932\n",
      " -----> Epoch: 227 \n",
      "--------->Loss: 0.0007765281944242365\n",
      " -----> Epoch: 228 \n",
      "--------->Loss: 0.001213977233552329\n",
      " -----> Epoch: 229 \n",
      "--------->Loss: 0.0007503645100135798\n",
      " -----> Epoch: 230 \n",
      "--------->Loss: 0.001189923762228622\n",
      " -----> Epoch: 231 \n",
      "--------->Loss: 0.0007339677665783314\n",
      " -----> Epoch: 232 \n",
      "--------->Loss: 0.001166547231273256\n",
      " -----> Epoch: 233 \n",
      "--------->Loss: 0.0007180719218387913\n",
      " -----> Epoch: 234 \n",
      "--------->Loss: 0.0011438232346992917\n",
      " -----> Epoch: 235 \n",
      "--------->Loss: 0.0007026575664321454\n",
      " -----> Epoch: 236 \n",
      "--------->Loss: 0.0011217284309491548\n",
      " -----> Epoch: 237 \n",
      "--------->Loss: 0.000687706194948592\n",
      " -----> Epoch: 238 \n",
      "--------->Loss: 0.0011002404885585226\n",
      " -----> Epoch: 239 \n",
      "--------->Loss: 0.0006732001570263481\n",
      " -----> Epoch: 240 \n",
      "--------->Loss: 0.0010403275272682709\n",
      " -----> Epoch: 241 \n",
      "--------->Loss: 0.00067145223251115\n",
      " -----> Epoch: 242 \n",
      "--------->Loss: 0.0010590486360207581\n",
      " -----> Epoch: 243 \n",
      "--------->Loss: 0.0006454493897883596\n",
      " -----> Epoch: 244 \n",
      "--------->Loss: 0.0009860148582708059\n",
      " -----> Epoch: 245 \n",
      "--------->Loss: 0.0006440497128487438\n",
      " -----> Epoch: 246 \n",
      "--------->Loss: 0.0010835713755972523\n",
      " -----> Epoch: 247 \n",
      "--------->Loss: 0.0006093395839258424\n",
      " -----> Epoch: 248 \n",
      "--------->Loss: 0.0008307189878491543\n",
      " -----> Epoch: 249 \n",
      "--------->Loss: 0.0006804329730749786\n",
      " -----> Epoch: 250 \n",
      "--------->Loss: 0.0009831354448924126\n",
      " -----> Epoch: 251 \n",
      "--------->Loss: 0.0005921534688652028\n",
      " -----> Epoch: 252 \n",
      "--------->Loss: 0.0008500800182637382\n",
      " -----> Epoch: 253 \n",
      "--------->Loss: 0.0006249078832682224\n",
      " -----> Epoch: 254 \n",
      "--------->Loss: 0.001008605559615015\n",
      " -----> Epoch: 255 \n",
      "--------->Loss: 0.0005620381768370202\n",
      " -----> Epoch: 256 \n",
      "--------->Loss: 0.0007507519538396482\n",
      " -----> Epoch: 257 \n",
      "--------->Loss: 0.0006532858930510518\n",
      " -----> Epoch: 258 \n",
      "--------->Loss: 0.0007553633056083178\n",
      " -----> Epoch: 259 \n",
      "--------->Loss: 0.000638677023770982\n",
      " -----> Epoch: 260 \n",
      "--------->Loss: 0.000741363525049422\n",
      " -----> Epoch: 261 \n",
      "--------->Loss: 0.0006015855532977088\n",
      " -----> Epoch: 262 \n",
      "--------->Loss: 0.0008833108730614391\n",
      " -----> Epoch: 263 \n",
      "--------->Loss: 0.0005260908825480397\n",
      " -----> Epoch: 264 \n",
      "--------->Loss: 0.0007612855239306575\n",
      " -----> Epoch: 265 \n",
      "--------->Loss: 0.000548410527573213\n",
      " -----> Epoch: 266 \n",
      "--------->Loss: 0.0009595378508849722\n",
      " -----> Epoch: 267 \n",
      "--------->Loss: 0.0004960890256710576\n",
      " -----> Epoch: 268 \n",
      "--------->Loss: 0.0005966286761930955\n",
      " -----> Epoch: 269 \n",
      "--------->Loss: 0.000689300550825634\n",
      " -----> Epoch: 270 \n",
      "--------->Loss: 0.00055283527544054\n",
      " -----> Epoch: 271 \n",
      "--------->Loss: 0.0008179151405826781\n",
      " -----> Epoch: 272 \n",
      "--------->Loss: 0.0004827967815997161\n",
      " -----> Epoch: 273 \n",
      "--------->Loss: 0.0007033093466390539\n",
      " -----> Epoch: 274 \n",
      "--------->Loss: 0.000503462854831164\n",
      " -----> Epoch: 275 \n",
      "--------->Loss: 0.0008444974145575322\n",
      " -----> Epoch: 276 \n",
      "--------->Loss: 0.0004596545904887153\n",
      " -----> Epoch: 277 \n",
      "--------->Loss: 0.0005885736497944957\n",
      " -----> Epoch: 278 \n",
      "--------->Loss: 0.000547945152842728\n",
      " -----> Epoch: 279 \n",
      "--------->Loss: 0.0006681446021644889\n",
      " -----> Epoch: 280 \n",
      "--------->Loss: 0.00048402725036426723\n",
      " -----> Epoch: 281 \n",
      "--------->Loss: 0.000752975352040657\n",
      " -----> Epoch: 282 \n",
      "--------->Loss: 0.0004464183322234542\n",
      " -----> Epoch: 283 \n",
      "--------->Loss: 0.0006046139604258919\n",
      " -----> Epoch: 284 \n",
      "--------->Loss: 0.00048702492571650477\n",
      " -----> Epoch: 285 \n",
      "--------->Loss: 0.0006994792312173662\n",
      " -----> Epoch: 286 \n",
      "--------->Loss: 0.00043523549209766195\n",
      " -----> Epoch: 287 \n",
      "--------->Loss: 0.0006006945192146293\n",
      " -----> Epoch: 288 \n",
      "--------->Loss: 0.0004625727997938574\n",
      " -----> Epoch: 289 \n",
      "--------->Loss: 0.0007061341029625553\n",
      " -----> Epoch: 290 \n",
      "--------->Loss: 0.00041169701602671623\n",
      " -----> Epoch: 291 \n",
      "--------->Loss: 0.0005656139419443166\n",
      " -----> Epoch: 292 \n",
      "--------->Loss: 0.00046297491295631466\n",
      " -----> Epoch: 293 \n",
      "--------->Loss: 0.0005725782890585429\n",
      " -----> Epoch: 294 \n",
      "--------->Loss: 0.0004394291984614396\n",
      " -----> Epoch: 295 \n",
      "--------->Loss: 0.0006739266635650882\n",
      " -----> Epoch: 296 \n",
      "--------->Loss: 0.0003903311164851002\n",
      " -----> Epoch: 297 \n",
      "--------->Loss: 0.0005386650723133707\n",
      " -----> Epoch: 298 \n",
      "--------->Loss: 0.00043223875243799596\n",
      " -----> Epoch: 299 \n",
      "--------->Loss: 0.0006536221037359919\n"
     ]
    }
   ],
   "source": [
    "def load_image(path, img_size=(64,64)): \n",
    "    images = []\n",
    "    labels = []\n",
    "    for label, class_dir in enumerate( ['cat' , 'dog'] ):\n",
    "        class_path = os.path.join(path, class_dir)\n",
    "\n",
    "        for img_name in os.listdir(class_path):\n",
    "            img_path = os.path.join(class_path, img_name)\n",
    "            img = Image.open(img_path).convert('L').resize(img_size)\n",
    "            images.append(np.array(img).flatten())\n",
    "            labels.append(label)\n",
    "\n",
    "    return np.array(images) , np.array(labels)\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1- sigmoid(x))\n",
    "\n",
    "class DenseLayer:\n",
    "    def __init__(self, input_size, output_size): #input_size = no. of rows ------ # output = no. of output neurons\n",
    "        self.weights = np.random.randn(input_size, output_size) * 0.1\n",
    "        self.bias = np.zeros((1, output_size))\n",
    "\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.input = inputs\n",
    "        self.output = sigmoid(np.dot(inputs, self.weights) + self.bias )\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, dvalues):\n",
    "        self.dweights = np.dot(self.input.T, dvalues)\n",
    "        self.dbias = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "        return self.dinputs\n",
    "    \n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, image_size_flatten):\n",
    "        self.layer1 = DenseLayer(image_size_flatten, 128)\n",
    "        self.layer2 = DenseLayer(128, 64)\n",
    "        self.layer3 = DenseLayer(64,1)\n",
    "\n",
    "    def forward(self, image_dataset_array):\n",
    "        out = self.layer1.forward(image_dataset_array)              # self.inputs: (9, 4096)     self.output: (9, 128)  image_dataset_array: (9, 4096)\n",
    "        out = self.layer2.forward(out)                              # self.inputs: (9, 128)      self.output: (9, 64)   image_dataset_array: (9, 128)\n",
    "        out = self.layer3.forward(out)                              # self.inputs: (9, 64)       self.output: (9, 1)    image_dataset_array: (9, 64\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dvalues):\n",
    "        dvalues = self.layer3.backward(dvalues)                     # self.dvalues: (9, 1)      self.dweights: (64, 1)      OUTPUT: (9, 64)\n",
    "        dvalues = self.layer2.backward(dvalues)                     # self.dvalues: (9, 64)     self.dweights: (128, 64)    OUTPUT: (9, 128)\n",
    "        self.layer1.backward(dvalues)                               # self.dvalues: (9, 128)    self.dweights: (4096, 128)  OUTPUT: (9, 4096)\n",
    "\n",
    "    def update(self, learning_rate):\n",
    "        self.layer1.weights -= learning_rate * self.layer1.dweights\n",
    "        self.layer1.bias -= learning_rate * self.layer1.dbias\n",
    "        \n",
    "        self.layer2.weights -= learning_rate * self.layer2.dweights\n",
    "        self.layer2.bias -= learning_rate * self.layer2.dbias\n",
    "        \n",
    "        self.layer3.weights -= learning_rate * self.layer3.dweights\n",
    "        self.layer3.bias -= learning_rate * self.layer3.dbias\n",
    "\n",
    "\n",
    "def train(model, image_dataset_array, actual_labels, epochs, learning_rate):\n",
    "    for epoch in range(epochs):\n",
    "        output = model.forward(image_dataset_array)\n",
    "        loss = np.mean((output-actual_labels)**2 )\n",
    "        print(f\" -----> Epoch: {epoch} \\n--------->Loss: {loss}\")\n",
    "\n",
    "        dvalues = 2* (output-actual_labels) / actual_labels.size\n",
    "        model.backward(dvalues)\n",
    "        model.update(learning_rate)\n",
    "\n",
    "\n",
    "\n",
    "image_dataset_array, actual_labels = load_image('dataset')\n",
    "actual_labels = actual_labels.reshape(-1, 1)\n",
    "\n",
    "input_image_flat_size = image_dataset_array.shape[1]\n",
    "\n",
    "\n",
    "\n",
    "model1 = NeuralNetwork(image_size_flatten=input_image_flat_size)\n",
    "\n",
    "\n",
    "train(model1, image_dataset_array, actual_labels, epochs=300, learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 40.00%\n",
      "Number of images - 5\n",
      "Precision: 0.40\n",
      "Recall: 1.00\n",
      "F1 Score: 0.57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gchakshu\\AppData\\Local\\Temp\\ipykernel_3456\\3745664846.py:17: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-x))\n"
     ]
    }
   ],
   "source": [
    "def load_mixed_images(path, img_size=(64, 64)):\n",
    "    images = []\n",
    "    actual_labels = []\n",
    "    for img_name in os.listdir(path):\n",
    "        img_path = os.path.join(path, img_name)\n",
    "        img = Image.open(img_path).convert('L').resize(img_size)\n",
    "        images.append(np.array(img).flatten())\n",
    "        label = 1 if 'dog' in img_name.lower() else 0  # Assuming naming convention indicates label\n",
    "        actual_labels.append(label)\n",
    "    return np.array(images), np.array(actual_labels)\n",
    "\n",
    "# Define a function to calculate accuracy\n",
    "def calculate_accuracy(predictions, actual_labels):\n",
    "    correct_predictions = (predictions.flatten() > 0.5).astype(int) == actual_labels\n",
    "    accuracy = np.mean(correct_predictions)\n",
    "    return accuracy\n",
    "\n",
    "# Define a function for precision, recall, and F1-score\n",
    "def calculate_precision_recall_f1(predictions, actual_labels):\n",
    "    predictions_binary = (predictions.flatten() > 0.5).astype(int)\n",
    "    true_positives = np.sum((predictions_binary == 1) & (actual_labels == 1))\n",
    "    false_positives = np.sum((predictions_binary == 1) & (actual_labels == 0))\n",
    "    false_negatives = np.sum((predictions_binary == 0) & (actual_labels == 1))\n",
    "\n",
    "    precision = true_positives / (true_positives + false_positives)\n",
    "    recall = true_positives / (true_positives + false_negatives)\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    return precision, recall, f1_score\n",
    "\n",
    "# Load the mixed dataset\n",
    "X_mixed, y_actual = load_mixed_images('test_set')\n",
    "\n",
    "# Get model predictions\n",
    "predictions = model1.forward(X_mixed)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = calculate_accuracy(predictions, y_actual)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "# Calculate precision, recall, and F1-score\n",
    "precision, recall, f1_score = calculate_precision_recall_f1(predictions, y_actual)\n",
    "print(f\"Number of images - {sum(os.path.isfile(os.path.join('test_set', f)) for f in os.listdir('test_set'))}\")\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'F1 Score: {f1_score:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CleanNNenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
